# Clustering Analysis on Three Datasets
This project applies K-Means, DBSCAN, and Hierarchical clustering to Wine, Customer, and Iris datasets. It compares algorithm performance, identifies key attributes, and emphasizes the importance of visualization in validating and interpreting clusters. Ideal for exploring clustering techniques in various contexts.
## Introduction
This project has been conducted by Savvas Theocharous and Stefanos Christou. The goal is to use three different datasets and three different clustering algorithms to compare, experiment and evaluate to better understand the way they work and find which is better and in what circumstances. Our results have been as expected. We have made some hypotheses before running the algorithms and that has helped us to better guide our work flow and it has been quite rewarding seeing the results and insights that couldn't have been spotted otherwise. By exhausting parameters to find the best clusters we have seen also why they work after using visualisations. Visualisations not only helped with parameters, but shed a light to the most important attributes and what have the most significant factor.
## Datasets
The three datasets that we have used are a Wine Dataset, a Customer Dataset and a Retail Invoice Dataset. They are all datasets that we can handle and understand the nature of, but need the power of a machine to cluster them into interesting and meaningful groups that can give us ideas and images we couldnâ€™t otherwise.
	Our first dataset is the Wine Dataset. It is the most popular dataset for such applications and quite easy to comprehend so that's why we have chosen it. These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. Using K-Means clustering we are going to try and see if the optimal K is 3 using elbow method as the wines are derived from three different cultivars and if the machine can actually cluster these wines correctly and what variable stands out the most as separators. The 13 attributes are Alcohol, Malic acid, Ash, Alcalinity of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, Proline. The number of instances is 178.
	Our Second dataset is an Income and Spending Dataset. It contains the ID, Gender, Region (Urban or Rural), Income in thousands per month and spending in thousands per month with the number of instances being 1113. We have chosen to apply the Hierarchical clustering algorithm on this data set as we can narrow it to two dimensional using only Income and Spending to create clusters of people. Our plan is to create a dendrogram to see what the machine will create and go from there. As a follow up we will cut the tree and label each person to visualise the clusters.
	Our final dataset is the Iris Dataset. For a DBSCAN we have found it quite challenging to choose a dataset, as it is quite difficult to visualise and validate DBSCAN Clusterings when the dataset is not two dimensional, but we came up with the idea to plot each pair and visualise each pair separately. The dataset includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other. The columns in the dataset are ID, Sepal Length, Sepal Width, Petal Length, Petal Width and Species. We have dropped the columns ID and Species so we can continue on clustering using only the 4 important attributes and hopefully separate the species using the DBSCAN clustering algorithm.
## Clustering Algorithms
DBSCAN is particularly useful for identifying clusters in datasets of varying shapes and densities. It defines clusters based on the density of data points. A core point is a data point with a minimum number of neighbours within a specified distance (eps). Border points are within the distance of a core point but have fewer neighbours, and outliers are points that don't meet the criteria for core or border points. The algorithm begins by selecting a random point and expanding the cluster by connecting neighbouring points until the density falls below the specified threshold. DBSCAN has the advantage of handling noise and discovering clusters of arbitrary shapes.

![image](https://github.com/user-attachments/assets/57686c21-479b-4ceb-89bc-b73b7679c524)

K-Means is a popular centroid-based clustering algorithm that aims to partition data into K clusters. It starts by randomly selecting K centroids, each representing a cluster. Data points are then assigned to the nearest centroid, and the centroids are recalculated based on the assigned points. This assignment and recalculation process repeats until convergence. K-Means seeks to minimise the sum of squared distances between data points and their assigned centroids. It's a versatile algorithm, but its performance can be influenced by the initial placement of centroids, making it sensitive to outliers.

![image](https://github.com/user-attachments/assets/8eebc920-a06c-4cd5-8da2-d29062762bde)

Hierarchical clustering builds a tree-like structure of clusters, often represented as a dendrogram. There are two main types: agglomerative, where each data point starts as a separate cluster and is iteratively merged based on proximity, and divisive, where all data points begin in one cluster and are recursively split. Agglomerative hierarchical clustering is more common. It starts by considering each data point as a separate cluster and then merges the closest clusters iteratively until only one cluster remains. The dendrogram visually represents the merging process, and the desired number of clusters is determined by cutting the tree at a specific level. Hierarchical clustering captures the hierarchical relationships between clusters, providing insights into both fine and coarse structures within the data.

![image](https://github.com/user-attachments/assets/a9f16f85-1e33-4c0f-a516-ee85b78dbc1a)

## Results and Discussion
For our Wine Dataset when we knew from the dataset description that the wines were cultivated from 3 different regions it led us to a hypothesis that there would be 3 different clusters. What better way to test that than the elbow method and K-Means clustering algorithms. Elbow Method is a technique used in unsupervised machine learning to determine the optimal number of clusters for a dataset. In this specific case, the dataset is related to wine characteristics. The code iteratively applies the K-means clustering algorithm for different numbers of clusters, ranging from 1 to 10. For each iteration, it calculates a metric called the "inertia," represented by the variable 'cs.' The inertia measures the sum of squared distances of data points to their assigned cluster centroids. 

![image](https://github.com/user-attachments/assets/cdfc7498-0cab-4a85-9545-60394075f73c)

The plot generated displays the number of clusters on the x-axis and the corresponding inertia values on the y-axis. The "elbow" in the plot indicates a point where adding more clusters does not significantly decrease the inertia. We can visually inspect the plot and identify the elbow point, which suggests to us the optimal number of clusters is 3. The same number of different regions the wine has been cultivated from.
	Now lets see what separates these 3 vineyards as we have all the characteristics of each wine. We will create a scatter plot for each pair of attributes with each cluster having a different colour.

![image](https://github.com/user-attachments/assets/e5cacb67-96bc-4f85-96d9-5e1443a14567)

This collection of plots may be a bit cluttered and not easy to read, but there is one thing we care about and it sticks out more than anything. All the plots that involve Proline the clusters are almost perfectly separated. This indicates that the most important factor to differentiate each cluster is the Proline attribute.
	For our next dataset for Customers we have applied the hierarchical clustering algorithm. We have narrowed the dataset to two dimensions leaving us with only Income and Spending as they are the most important attributes to analyse in our judgement. We have changed the categorical values for region and gender into numerical binary format as a beginning. Then we have started plotting dendrograms. In this algorithm we have experimented with the linkage parameter. Using average and complete linkage has shown promising results and both have seen to produce nice hierarchy clusters. For both we see 3 main clusters.

![image](https://github.com/user-attachments/assets/52071b75-6df8-4f89-932e-e01bfaf404ca)

The dendrogram itself doesn't show us much so we followed by cutting the tree into the 3 different clusters and labelling the entries. This allowed us to be able to visualise the clusters by plotting scatter plots with Income and Spending labels to see which areas each cluster is located. This will give us an idea on what terms each cluster is separated and what each one means.

![image](https://github.com/user-attachments/assets/2cb4c47b-5d4b-4419-bfb0-35543c2a068d)

The results are pretty straightforward. Cluster 0 is low class, Cluster 1 is middle class and Cluster 2 is high class people based on their incomes. Since the scatter plot is filled the Spending unfortunately is negligible as they all vary in spending habits across the whole axis.
	For our last dataset for Iris we have used the DBSCAN. First step that we have felt the need to do is look at all the scatter plots of each pair for all the Iris attributes. As DBSCAN Clusters using distance and neighbours which can also be seen with the eye visualising those plots would help us come up with a hypothesis and have an expectation of how the DBSCAN should work before we execute it.

![image](https://github.com/user-attachments/assets/b87ef91f-8235-42a9-a583-b670b0c97c39)

Looking at these plots we can see clear clusters and for some attributes they are more clear than others. We will use the algorithm experiment with parameters to get the most appropriate clusters and see the outliers it comes up with.

![image](https://github.com/user-attachments/assets/5351f331-d613-4ecb-9ff0-c07d20e9d5aa)

The DBSCAN has worked very well clustering the points and it is very easy to validate by the eye without using any extensive process. One thing that can be said about Iris looking at these clusters is how the petal is the more significant factor to identifying the species as there are less outliers and more distinct clusters further away from each other showing high cluster separation.
## Conclusion
In our comprehensive exploration of clustering techniques across diverse datasets, we employed the Elbow Method and K-Means clustering to identify distinct vineyards in the Wine dataset, successfully validating our hypothesis of three clusters. The significance of the Proline attribute in separating these clusters became evident through scatter plots. Shifting focus to the Customer dataset, hierarchical clustering based on Income and Spending revealed clear distinctions, classifying individuals into low, middle, and high-income clusters. For the Iris dataset, DBSCAN efficiently identified natural groupings, with clear clusters and minimal outliers observed in scatter plots. Notably, the petal attributes emerged as crucial for species identification, reflecting high cluster separation. Overall, our application of clustering algorithms provided valuable insights into the inherent structures of each dataset, showcasing the versatility and effectiveness of these unsupervised learning techniques. Parameters have to be experimented with as they cannot be a one fit for all and using validation systems such Elbow Method can be of help even though it is not enough and validation could be the most difficult task of all when dealing with clustering algorithms. Our take away from this whole project is that visualisation is very important. Being able to understand the data and see it clearly using plots and graphs is key to validate the clusters being generated and understand them as well. The clustering algorithms can create the clusters, but they have no value until you are able to visualise and comprehend how the machine created those clusters.






